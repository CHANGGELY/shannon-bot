#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Tardis æ•°æ® ETL è„šæœ¬ (é«˜å…¼å®¹ç‰ˆ)
åŠŸèƒ½ï¼šä½¿ç”¨ requests ä¸‹è½½å¢é‡ L2 æ•°æ®ï¼Œå¹¶ä½¿ç”¨ Polars è¿›è¡Œæè‡´å‹ç¼©è½¬æ¢ã€‚
"""

import os
import glob
import logging
import polars as pl
import requests
from datetime import datetime, timedelta
import argparse

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# === âš™ï¸ æ ¸å¿ƒé…ç½®åŒºåŸŸ ===
TARGET_SYMBOLS = ['BTCUSDT', 'ETHUSDT']
# ç²¾ç¡®çš„èµ·å§‹æ—¥æœŸé…ç½® (æ¥æºäº Tardis API)
SYMBOL_START_DATES = {
    'BTCUSDT': '2019-11-17',
    'ETHUSDT': '2019-11-27'
}
EXCHANGE = 'binance-futures'
DOWNLOAD_DIR = './tardis_temp'
OUTPUT_DIR = './final_parquet'

# ç²¾åº¦æ§åˆ¶
PRICE_MULT = 100  
AMOUNT_MULT = 1000 

def get_monthly_first_days(start_date_str: str) -> list[str]:
    start_date = datetime.strptime(start_date_str[:10], "%Y-%m-%d")
    end_date = datetime.now()
    dates = []
    curr = start_date.replace(day=1)
    if curr < start_date:
        if curr.month == 12: curr = curr.replace(year=curr.year+1, month=1)
        else: curr = curr.replace(month=curr.month+1)
    while curr <= end_date:
        dates.append(curr.strftime("%Y-%m-%d"))
        if curr.month == 12: curr = curr.replace(year=curr.year+1, month=1)
        else: curr = curr.replace(month=curr.month+1)
    return dates

def download_file(url: str, dest_path: str, max_retries: int = 5):
    """ä½¿ç”¨ requests ä¸‹è½½æ–‡ä»¶ï¼Œæ”¯æŒç¦ç”¨ SSL æ ¡éªŒå’Œé‡è¯•"""
    import time
    
    for attempt in range(1, max_retries + 1):
        try:
            logger.info(f"å¼€å§‹ä¸‹è½½ (å°è¯• {attempt}/{max_retries}): {url}")
            # verify=False å½»åº•è§£å†³è¯ä¹¦é—®é¢˜
            with requests.get(url, stream=True, timeout=300, verify=False) as r:
                r.raise_for_status()
                with open(dest_path, 'wb') as f:
                    for chunk in r.iter_content(chunk_size=1024*1024):
                        if chunk: f.write(chunk)
            return True
        except Exception as e:
            wait_time = 2 ** attempt # æŒ‡æ•°é€€é¿: 2, 4, 8, 16, 32 ç§’
            logger.warning(f"ä¸‹è½½å¤±è´¥ (å°è¯• {attempt}/{max_retries}): {e} | ç­‰å¾… {wait_time}s åé‡è¯•...")
            if attempt < max_retries:
                time.sleep(wait_time)
            else:
                logger.error(f"âŒ æœ€ç»ˆä¸‹è½½å¤±è´¥: {url}")
                return False
    return False

def process_and_compress(csv_path: str, symbol: str, date: str):
    output_filename = f"{symbol}_{date}_incremental.parquet"
    output_path = os.path.join(OUTPUT_DIR, output_filename)
    
    if os.path.exists(output_path):
        logger.info(f"âœ… {output_filename} å·²å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
        if os.path.exists(csv_path): os.remove(csv_path)
        return

    logger.info(f"ğŸ”„ æ­£åœ¨è½¬æ¢å‹ç¼©: {symbol} - {date} ...")
    try:
        # æ˜¾å¼æŒ‡å®š schema ä»¥é¿å…ç±»å‹æ¨æ–­é”™è¯¯ï¼ŒTardis CSV çš„ timestamp é€šå¸¸æ˜¯ i64 (å¾®ç§’)
        # ä½†æœ‰æ—¶ä¹Ÿå¯èƒ½æ˜¯ ISO å­—ç¬¦ä¸²ï¼Œæ‰€ä»¥æˆ‘ä»¬å…ˆä¸å¼ºåˆ¶ schemaï¼Œè€Œæ˜¯åœ¨è¡¨è¾¾å¼é‡Œå¤„ç†
        q = pl.scan_csv(csv_path)

        # 2. æè‡´å‹ç¼©è½¬æ¢é€»è¾‘
        # å…¼å®¹å¤„ç†ï¼šå¦‚æœ timestamp å·²ç»æ˜¯æ•°å­—ï¼Œç›´æ¥ cast ä¸º Datetimeï¼›å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå…ˆè½¬ Datetime
        # Polars çš„ cast(pl.Datetime("us")) å¯¹ i64 (å¾®ç§’) æ˜¯ç›´æ¥ç”Ÿæ•ˆçš„
        
        df = q.with_columns([
            # å…ˆç»Ÿä¸€å°è¯•è½¬ä¸º Int64 (å¾®ç§’)ï¼Œå¦‚æœåŸæœ¬æ˜¯ String æ ¼å¼çš„æ•°å­—ä¹Ÿèƒ½è½¬
            # å¦‚æœæ˜¯ ISO å­—ç¬¦ä¸²ï¼Œpl.col("timestamp").cast(pl.Int64) å¯èƒ½ä¼šå¤±è´¥ï¼Œ
            # ä½† Tardis çš„ incremental_book_L2 é»˜è®¤ç¡®å®æ˜¯å¾®ç§’æ•´æ•°
            pl.col("timestamp").cast(pl.Int64).cast(pl.Datetime("us")),
            pl.col("local_timestamp").cast(pl.Int64).cast(pl.Datetime("us")),
            
            # æ–‡æœ¬å‹ç¼©ï¼šé‡å¤çš„ symbol/side è½¬åˆ†ç±»ç¼–ç 
            pl.col("symbol").cast(pl.Categorical),
            pl.col("side").cast(pl.Categorical),
            pl.col("is_snapshot").cast(pl.Boolean),

            # æ ¸å¿ƒï¼šFloat è½¬ Int (æå‡å‹ç¼©ç‡)
            (pl.col("price") * PRICE_MULT).round(0).cast(pl.Int64).alias("price_int"),
            (pl.col("amount") * AMOUNT_MULT).round(0).cast(pl.Int64).alias("amount_int")
        ]).select([
            # åªä¿ç•™éœ€è¦çš„åˆ—
            "symbol", "timestamp", "local_timestamp", "is_snapshot", "side", "price_int", "amount_int"
        ]).collect()

        df.write_parquet(output_path, compression='zstd', compression_level=10, use_pyarrow=True)
        
        raw_size = os.path.getsize(csv_path) / (1024*1024)
        pq_size = os.path.getsize(output_path) / (1024*1024)
        logger.info(f"ğŸ‰ å®Œæˆ! {raw_size:.1f}MB -> {pq_size:.1f}MB (å‹ç¼©ç‡: {pq_size/raw_size:.1%})")
        os.remove(csv_path)
    except Exception as e:
        logger.error(f"âŒ å¤„ç†å‡ºé”™: {e}")

def process_task(task_info):
    """å•ä¸ªä»»åŠ¡çš„å¤„ç†é€»è¾‘ï¼šä¸‹è½½ -> è½¬æ¢"""
    symbol, date, output_dir, download_dir = task_info
    
    # æ„é€ è·¯å¾„
    final_path = os.path.join(output_dir, f"{symbol}_{date}_incremental.parquet")
    if os.path.exists(final_path):
        return f"âœ… {symbol} {date} å·²å®Œæˆ"

    yyyy, mm, dd = date.split('-')
    url = f"https://datasets.tardis.dev/v1/{EXCHANGE}/incremental_book_L2/{yyyy}/{mm}/{dd}/{symbol}.csv.gz"
    dest_csv = os.path.join(download_dir, f"{symbol}_{date}.csv.gz")

    # ä¸‹è½½
    if download_file(url, dest_csv):
        # è½¬æ¢
        try:
            process_and_compress(dest_csv, symbol, date)
            return f"ğŸ‰ å®Œæˆ {symbol} {date}"
        except Exception as e:
            return f"âŒ è½¬æ¢å¤±è´¥ {symbol} {date}: {e}"
    else:
        return f"âŒ ä¸‹è½½å¤±è´¥ {symbol} {date}"

def main():
    global DOWNLOAD_DIR, OUTPUT_DIR
    parser = argparse.ArgumentParser(description='Tardis Data ETL Tool')
    parser.add_argument('--symbols', nargs='+', default=TARGET_SYMBOLS)
    parser.add_argument('--download_dir', default=DOWNLOAD_DIR)
    parser.add_argument('--output_dir', default=OUTPUT_DIR)
    # æ–°å¢å¹¶å‘å‚æ•°
    parser.add_argument('--workers', type=int, default=4, help='å¹¶å‘ä¸‹è½½æ•°é‡')
    args = parser.parse_args()

    os.makedirs(args.download_dir, exist_ok=True)
    os.makedirs(args.output_dir, exist_ok=True)
    
    DOWNLOAD_DIR = args.download_dir
    OUTPUT_DIR = args.output_dir

    # 1. ç”Ÿæˆä»»åŠ¡åˆ—è¡¨
    tasks = []
    for symbol in args.symbols:
        start_date = SYMBOL_START_DATES.get(symbol, "2020-01-01")
        target_dates = get_monthly_first_days(start_date)
        for date in target_dates:
            tasks.append((symbol, date, OUTPUT_DIR, DOWNLOAD_DIR))
            
    total = len(tasks)
    logger.info(f"ğŸš€ å¯åŠ¨å¹¶è¡Œæ¨¡å¼ï¼ŒWorkers={args.workers}ï¼Œæ€»ä»»åŠ¡æ•°: {total}")

    # 2. å¹¶è¡Œæ‰§è¡Œ
    # ä½¿ç”¨ ThreadPoolExecutor è¿›è¡Œå¹¶å‘ä¸‹è½½
    # æ³¨æ„ï¼šè™½ç„¶æ˜¯å¤šçº¿ç¨‹ï¼Œä½† process_and_compress é‡Œçš„ Polars è¿ç®—ä¼šé‡Šæ”¾ GILï¼Œæ‰€ä»¥ä¹Ÿèƒ½åˆ©ç”¨å¤šæ ¸
    from concurrent.futures import ThreadPoolExecutor, as_completed
    
    with ThreadPoolExecutor(max_workers=args.workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_task = {executor.submit(process_task, t): t for t in tasks}
        
        completed_count = 0
        for future in as_completed(future_to_task):
            completed_count += 1
            res = future.result()
            progress = completed_count / total
            logger.info(f"[{completed_count}/{total} | {progress:.1%}] {res}")

if __name__ == "__main__":
    # ç¦ç”¨è­¦å‘Š
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    main()