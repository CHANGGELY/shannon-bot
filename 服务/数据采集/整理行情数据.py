import argparse
import json
import shutil
import subprocess
import sys
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Iterable, Literal, cast

import pandas as pd


# =================================================================
# âš™ï¸ å¿«é€Ÿé…ç½®åŒº (å¯ä»¥ç›´æ¥åœ¨è¿™é‡Œä¿®æ”¹å‚æ•°åç‚¹è¿è¡Œ)
# =================================================================

# 1. æ•´ç†å“ªäº›å¸ç§çš„æ•°æ®ï¼Ÿ(ä¾‹å¦‚ "BTCUSDC,ETHUSDC"ï¼Œç•™ç©ºåˆ™æ•´ç†æ‰€æœ‰å¸ç§)
é»˜è®¤_SYMBOLS = ""

# 2. æ•´ç†å“ªä¸ªæ—¥æœŸçš„æ•°æ®ï¼Ÿ(ä¾‹å¦‚ "2023-12-21"ï¼Œç•™ç©ºåˆ™æ•´ç†æ‰€æœ‰å†å²æ—¥æœŸ)
é»˜è®¤_DATE = ""

# 3. æ•´ç†æ—¶æ˜¯å¦è‡ªåŠ¨è¡¥å…¨æˆäº¤æ•°æ®ï¼Ÿ
# å¦‚æœè®¾ç½®ä¸º Trueï¼Œå½“ç¨‹åºå‘ç°æ·±åº¦æ•°æ®(Depth)æœ‰ç©ºç¼ºæ—¶ï¼Œä¼šè‡ªåŠ¨è°ƒç”¨ã€Šè¡¥å…¨å†å²æˆäº¤.pyã€‹å»è¡¥å…¨å¯¹åº”çš„æˆäº¤æ•°æ®(Trade)
é»˜è®¤_AUTO_FILL_TRADE_FROM_DEPTH_GAPS = True

# 4. åˆ¤å®šä¸ºâ€œç©ºç¼ºâ€çš„é˜ˆå€¼ (å•ä½: æ¯«ç§’)
# é»˜è®¤ 60,000 æ¯«ç§’ = 1 åˆ†é’Ÿã€‚å¦‚æœä¸¤è¡Œæ•°æ®ä¹‹é—´çš„æ—¶é—´å·®è¶…è¿‡è¿™ä¸ªå€¼ï¼Œå°±è®¤ä¸ºä¸­é—´æœ‰æ–­æ¡£ã€‚
é»˜è®¤_FILL_DEPTH_GAP_MIN_MS = 60_000

# 5. è·¯å¾„é…ç½® (é€šå¸¸ä¸éœ€è¦ä¿®æ”¹)
é»˜è®¤_INPUT = str(Path(__file__).resolve().parents[2] / "data" / "è¡Œæƒ…æ•°æ®")
é»˜è®¤_OUTPUT = str(Path(__file__).resolve().parents[2] / "data" / "è¡Œæƒ…æ•°æ®_æ•´ç†")
# é»˜è®¤å¤‡ä»½ç›®å½•ï¼šæ•´ç†å®Œæˆåï¼Œå°†åŸå§‹ç¢ç‰‡æ–‡ä»¶ç§»åŠ¨åˆ°è¿™é‡Œ (ç›¸å½“äºå½’æ¡£)ï¼Œè€Œä¸æ˜¯ç›´æ¥åˆ é™¤
é»˜è®¤_BACKUP_DIR = str(Path(__file__).resolve().parents[2] / "data" / "è¡Œæƒ…æ•°æ®_å¤‡ä»½")

# 6. å…¶ä»–é«˜çº§è®¾ç½®
é»˜è®¤_DTYPE = ""              # åªæ•´ç†ç‰¹å®šç±»å‹ (depth æˆ– trade)ï¼Œç•™ç©ºåˆ™å…¨éƒ¨æ•´ç†
é»˜è®¤_OVERWRITE = True       # å¦‚æœè¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ï¼Œæ˜¯å¦è¦†ç›–ï¼Ÿ
é»˜è®¤_MOVE_TO_BACKUP = True  # ã€æ¨èã€‘æ•´ç†åå°†ç¢ç‰‡æ–‡ä»¶ç§»åŠ¨åˆ°å¤‡ä»½ç›®å½• (é¿å…ä¸‹æ¬¡é‡å¤æ•´ç†ï¼Œä¸”æ¯”åˆ é™¤æ›´å®‰å…¨)
é»˜è®¤_DELETE_SOURCE = False   # (å·²å¼ƒç”¨ï¼Œå»ºè®®ç”¨ MOVE_TO_BACKUP) æ•´ç†å®Œåæ˜¯å¦åˆ é™¤åŸå§‹ç¢ç‰‡æ–‡ä»¶ï¼Ÿ
é»˜è®¤_DELETE_TODAY = False    # æ˜¯å¦ç§»åŠ¨/åˆ é™¤ä»Šå¤©çš„ç¢ç‰‡æ–‡ä»¶ï¼Ÿ(ä»Šå¤©çš„è¿˜åœ¨é‡‡é›†ï¼Œå»ºè®®ä¸ç§»åŠ¨)
é»˜è®¤_CHECK_GAP = True        # æ˜¯å¦æ£€æŸ¥å¹¶ç”Ÿæˆç©ºç¼ºæŠ¥å‘Šï¼Ÿ
é»˜è®¤_SYNC_HF = True          # æ•´ç†å®Œæˆåæ˜¯å¦è‡ªåŠ¨åŒæ­¥åˆ° Hugging Face Dataset
é»˜è®¤_GAP_MS_DEPTH = 2000     # æ·±åº¦æ•°æ®è¶…è¿‡ 2 ç§’æ²¡æ•°æ®å°±ç®—å°ç¼ºå£
é»˜è®¤_GAP_MS_TRADE = 10000    # æˆäº¤æ•°æ®è¶…è¿‡ 10 ç§’æ²¡æ•°æ®å°±ç®—å°ç¼ºå£
é»˜è®¤_GAP_SAMPLES = 50        # æ¯ä¸ªæ–‡ä»¶æœ€å¤šè®°å½•å¤šå°‘ä¸ªç¼ºå£æ ·æœ¬
é»˜è®¤_FILL_MAX_GAPS_PER_SYMBOL_DAY = 100 # æ¯ä¸ªå¸ç§æ¯å¤©æœ€å¤šè¡¥å…¨å¤šå°‘ä¸ªå¤§ç¼ºå£
é»˜è®¤_FILL_MAX_WINDOW_MS = 24 * 60 * 60 * 1000  # å•æ¬¡è¡¥å…¨æœ€å¤§è·¨åº¦ (é»˜è®¤ 24 å°æ—¶)

# =================================================================


æ•°æ®ç±»å‹ = Literal["depth", "trade"]


@dataclass(frozen=True)
class ç¼ºå£:
    symbol: str
    dtype: æ•°æ®ç±»å‹
    date: str
    prev_exchange_time: int
    next_exchange_time: int
    gap_ms: int


def _iter_input_files(input_root: Path, symbols: list[str] | None) -> Iterable[Path]:
    if not input_root.exists():
        return
    for symbol_dir in sorted([p for p in input_root.iterdir() if p.is_dir()]):
        symbol = symbol_dir.name
        if symbols and symbol not in symbols:
            continue
        for date_dir in sorted([p for p in symbol_dir.iterdir() if p.is_dir()]):
            for parquet_file in sorted(date_dir.glob("*.parquet")):
                yield parquet_file


def _parse_file_meta(p: Path) -> tuple[str, str, æ•°æ®ç±»å‹] | None:
    try:
        symbol = p.parents[1].name
        date = p.parent.name
        name = p.name
        if name.startswith("depth_"):
            return symbol, date, "depth"
        if name.startswith("trade_") or name.startswith("trade_hist_"):
            return symbol, date, "trade"
        return None
    except Exception:
        return None


def _read_parquet_safe(file_path: Path) -> pd.DataFrame | None:
    try:
        return pd.read_parquet(file_path)
    except Exception:
        return None


def _dedupe(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return df
    if "timestamp" in df.columns:
        dedupe_cols = [c for c in df.columns if c != "timestamp"]
        if dedupe_cols:
            df = df.sort_values(["exchange_time", "timestamp"], kind="stable").drop_duplicates(
                subset=dedupe_cols, keep="first"
            )
            return df
    return df.drop_duplicates(keep="first")


def _normalize_types(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return df
    if "exchange_time" in df.columns:
        df["exchange_time"] = pd.to_numeric(df["exchange_time"], errors="coerce").astype("Int64")
    if "timestamp" in df.columns:
        df["timestamp"] = pd.to_numeric(df["timestamp"], errors="coerce")
    return df.dropna(subset=[c for c in ["exchange_time", "symbol"] if c in df.columns]).copy()


def _check_gaps(
    df: pd.DataFrame,
    symbol: str,
    dtype: æ•°æ®ç±»å‹,
    date: str,
    gap_threshold_ms: int,
    max_samples: int,
) -> tuple[dict, list[ç¼ºå£]]:
    if df.empty or "exchange_time" not in df.columns:
        return {"gap_threshold_ms": gap_threshold_ms, "gap_count": 0, "max_gap_ms": 0}, []

    s = df["exchange_time"].astype("int64", errors="ignore")
    s = s.sort_values(kind="stable").reset_index(drop=True)
    diff = s.diff().fillna(0).astype("int64")
    gap_mask = diff > int(gap_threshold_ms)
    gap_count = int(gap_mask.sum())
    max_gap = int(diff.max()) if len(diff) else 0

    gaps: list[ç¼ºå£] = []
    if gap_count:
        idxs = gap_mask[gap_mask].index.tolist()[:max_samples]
        for i in idxs:
            prev_t = int(s.iloc[i - 1])
            next_t = int(s.iloc[i])
            gaps.append(
                ç¼ºå£(
                    symbol=symbol,
                    dtype=dtype,
                    date=date,
                    prev_exchange_time=prev_t,
                    next_exchange_time=next_t,
                    gap_ms=int(next_t - prev_t),
                )
            )

    summary = {
        "gap_threshold_ms": int(gap_threshold_ms),
        "gap_count": gap_count,
        "max_gap_ms": max_gap,
    }
    return summary, gaps


def _write_output(df: pd.DataFrame, out_file: Path, overwrite: bool) -> None:
    out_file.parent.mkdir(parents=True, exist_ok=True)
    if out_file.exists() and not overwrite:
        return
    df.to_parquet(out_file, engine="pyarrow", compression="snappy", index=False)


def _split_csv(s: str) -> list[str] | None:
    items = [x.strip() for x in str(s or "").split(",") if x.strip()]
    return items or None


def _iter_input_files_with_filters(
    input_root: Path,
    symbols: list[str] | None,
    date_filter: str | None,
    dtype_filter: æ•°æ®ç±»å‹ | None,
) -> Iterable[Path]:
    for p in _iter_input_files(input_root, symbols):
        meta = _parse_file_meta(p)
        if not meta:
            continue
        _, date, dtype = meta
        if date_filter and date != date_filter:
            continue
        if dtype_filter and dtype != dtype_filter:
            continue
        yield p


def _build_groups(
    input_root: Path,
    symbols: list[str] | None,
    date_filter: str | None,
    dtype_filter: æ•°æ®ç±»å‹ | None,
) -> dict[tuple[str, str, æ•°æ®ç±»å‹], list[Path]]:
    groups: dict[tuple[str, str, æ•°æ®ç±»å‹], list[Path]] = {}
    for f in _iter_input_files_with_filters(input_root, symbols, date_filter, dtype_filter):
        meta = _parse_file_meta(f)
        if not meta:
            continue
        symbol, date, dtype = meta
        groups.setdefault((symbol, date, dtype), []).append(f)
    return groups


def _run_fill_trade(symbol: str, start_ms: int, end_ms: int) -> int:
    if start_ms >= end_ms:
        return 0
    cmd = [
        sys.executable,
        str(Path(__file__).resolve().parent / "è¡¥å…¨å†å²æˆäº¤.py"),
        "--symbol",
        str(symbol),
        "--start-ms",
        str(int(start_ms)),
        "--end-ms",
        str(int(end_ms)),
    ]
    p = subprocess.run(cmd, check=False)
    return int(p.returncode or 0)


def _plan_fill_from_depth_gaps(
    gap_samples: list[dict],
    min_gap_ms: int,
    max_gaps_per_symbol_day: int,
    max_window_ms: int,
) -> list[dict]:
    candidates: dict[tuple[str, str], list[dict]] = {}
    for g in gap_samples or []:
        try:
            if g.get("dtype") != "depth":
                continue
            if int(g.get("gap_ms", 0)) < int(min_gap_ms):
                continue
            symbol = str(g.get("symbol") or "")
            date = str(g.get("date") or "")
            if not symbol or not date:
                continue
            candidates.setdefault((symbol, date), []).append(g)
        except Exception:
            continue

    plans: list[dict] = []
    for (symbol, date), gaps in sorted(candidates.items()):
        gaps_sorted = sorted(gaps, key=lambda x: int(x.get("gap_ms", 0)), reverse=True)
        for g in gaps_sorted[: int(max_gaps_per_symbol_day)]:
            try:
                start_ms = int(g["prev_exchange_time"]) + 1
                end_ms = int(g["next_exchange_time"]) - 1
                if end_ms - start_ms > int(max_window_ms):
                    end_ms = start_ms + int(max_window_ms)
                if start_ms >= end_ms:
                    continue
                plans.append(
                    {
                        "symbol": symbol,
                        "date": date,
                        "start_ms": start_ms,
                        "end_ms": end_ms,
                        "gap_ms": int(g.get("gap_ms", 0)),
                    }
                )
            except Exception:
                continue
    return plans


def _organize_groups(
    groups: dict[tuple[str, str, æ•°æ®ç±»å‹], list[Path]],
    output_root: Path,
    overwrite: bool,
    delete_source: bool,
    move_to_backup: bool,
    backup_root: Path,
    delete_today: bool,
    check_gap: bool,
    gap_ms_depth: int,
    gap_ms_trade: int,
    gap_samples_limit: int,
    report_groups_by_key: dict[tuple[str, str, æ•°æ®ç±»å‹], dict],
    gap_summaries_by_key: dict[tuple[str, str, æ•°æ®ç±»å‹], dict],
    gap_samples_by_key: dict[tuple[str, str, æ•°æ®ç±»å‹], list[dict]],
) -> None:
    today_str = datetime.now().strftime("%Y-%m-%d")

    for (symbol, date, dtype), files in sorted(groups.items()):
        dfs: list[pd.DataFrame] = []
        bad_files: list[str] = []

        out_path = output_root / symbol / date / f"{dtype}.parquet"

        # 1. å°è¯•è¯»å–å·²å­˜åœ¨çš„è¾“å‡ºæ–‡ä»¶ (æ”¯æŒå¢é‡åˆå¹¶)
        if out_path.exists() and out_path.stat().st_size > 0:
            try:
                df_existing = pd.read_parquet(out_path)
                if not df_existing.empty:
                    dfs.append(df_existing)
            except Exception:
                pass  # å¦‚æœæ—§æ–‡ä»¶æŸåï¼Œå°±å¿½ç•¥å®ƒï¼Œé‡æ–°ç”Ÿæˆ

        # 2. è¯»å–æ–°çš„ç¢ç‰‡æ–‡ä»¶
        for p in sorted(files):
            df = _read_parquet_safe(p)
            if df is None:
                bad_files.append(str(p))
                continue
            dfs.append(df)

        if not dfs:
            report_groups_by_key[(symbol, date, dtype)] = {
                "symbol": symbol,
                "date": date,
                "dtype": dtype,
                "input_files": len(files),
                "bad_files": bad_files,
                "output": None,
                "rows": 0,
                "deleted_files": [],
                "moved_files": [],
                "delete_errors": [],
                "delete_skipped_reason": None,
            }
            gap_summaries_by_key.pop((symbol, date, dtype), None)
            gap_samples_by_key.pop((symbol, date, dtype), None)
            continue

        df_all = pd.concat(dfs, ignore_index=True)
        df_all = _normalize_types(df_all)
        
        # å³ä½¿æœ€ç»ˆä¸ºç©ºï¼Œä¹Ÿå¯èƒ½éœ€è¦å†™å…¥ç©ºæ–‡ä»¶æˆ–è®°å½•
        if df_all.empty:
            _write_output(df_all, out_path, overwrite=overwrite)
            report_groups_by_key[(symbol, date, dtype)] = {
                "symbol": symbol,
                "date": date,
                "dtype": dtype,
                "input_files": len(files),
                "bad_files": bad_files,
                "output": str(out_path),
                "rows": 0,
                "deleted_files": [],
                "moved_files": [],
                "delete_errors": [],
                "delete_skipped_reason": None,
            }
            gap_summaries_by_key.pop((symbol, date, dtype), None)
            gap_samples_by_key.pop((symbol, date, dtype), None)
            continue

        sort_cols = [c for c in ["exchange_time", "timestamp"] if c in df_all.columns]
        if sort_cols:
            df_all = df_all.sort_values(sort_cols, kind="stable").reset_index(drop=True)
        df_all = _dedupe(df_all)

        _write_output(df_all, out_path, overwrite=True) # æ€»æ˜¯ overwriteï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»åˆå¹¶äº†æ—§æ•°æ®

        delete_skipped_reason = None
        deleted_files: list[str] = []
        moved_files: list[str] = []
        delete_errors: list[str] = []

        # 3. å¤„ç†æºæ–‡ä»¶ (ç§»åŠ¨åˆ°å¤‡ä»½ æˆ– åˆ é™¤)
        if (move_to_backup or delete_source) and not bad_files:
            if date == today_str and not delete_today:
                delete_skipped_reason = "today"
            elif not out_path.exists():
                delete_skipped_reason = "no_output"
            else:
                for p in sorted(files):
                    try:
                        if move_to_backup:
                            # ç§»åŠ¨é€»è¾‘
                            # ç›®æ ‡è·¯å¾„: backup_root / symbol / date / filename
                            backup_path = backup_root / symbol / date / p.name
                            backup_path.parent.mkdir(parents=True, exist_ok=True)
                            shutil.move(str(p), str(backup_path))
                            moved_files.append(str(p))
                        elif delete_source:
                            # åˆ é™¤é€»è¾‘
                            p.unlink(missing_ok=True)
                            deleted_files.append(str(p))
                    except Exception as e:
                        delete_errors.append(f"{p}: {e}")

        report_groups_by_key[(symbol, date, dtype)] = {
            "symbol": symbol,
            "date": date,
            "dtype": dtype,
            "input_files": len(files),
            "bad_files": bad_files,
            "output": str(out_path),
            "rows": int(len(df_all)),
            "deleted_files": deleted_files,
            "moved_files": moved_files,
            "delete_errors": delete_errors,
            "delete_skipped_reason": delete_skipped_reason,
        }

        if check_gap:
            threshold = int(gap_ms_depth) if dtype == "depth" else int(gap_ms_trade)
            summary, gaps = _check_gaps(
                df_all,
                symbol=symbol,
                dtype=dtype,
                date=date,
                gap_threshold_ms=int(threshold),
                max_samples=int(gap_samples_limit),
            )
            gap_summaries_by_key[(symbol, date, dtype)] = {
                "symbol": symbol,
                "date": date,
                "dtype": dtype,
                **summary,
            }
            gap_samples_by_key[(symbol, date, dtype)] = [g.__dict__ for g in gaps]
        else:
            gap_summaries_by_key.pop((symbol, date, dtype), None)
            gap_samples_by_key.pop((symbol, date, dtype), None)

        print(f"æ•´ç†å®Œæˆ {symbol} {date} {dtype}: è¾“å…¥{len(files)}ä¸ªæ–‡ä»¶ -> {out_path.name}, è¡Œæ•°={len(df_all)}")


def main(argv: list[str]) -> int:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--input",
        default=é»˜è®¤_INPUT,
    )
    parser.add_argument(
        "--output",
        default=é»˜è®¤_OUTPUT,
    )
    parser.add_argument(
        "--backup-dir",
        default=é»˜è®¤_BACKUP_DIR,
        help="æ•´ç†åç¢ç‰‡æ–‡ä»¶çš„å¤‡ä»½ç›®å½•",
    )
    parser.add_argument("--symbols", default=é»˜è®¤_SYMBOLS, help="é€—å·åˆ†éš”ï¼Œå¦‚ BTCUSDC,ETHUSDC")
    parser.add_argument("--date", default=é»˜è®¤_DATE, help="YYYY-MM-DDï¼Œç•™ç©ºè¡¨ç¤ºæ‰€æœ‰æ—¥æœŸ")
    parser.add_argument(
        "--dtype",
        default=é»˜è®¤_DTYPE,
        choices=["", "depth", "trade"],
    )
    parser.add_argument("--overwrite", action="store_true", default=bool(é»˜è®¤_OVERWRITE))
    parser.add_argument("--move-to-backup", action="store_true", default=bool(é»˜è®¤_MOVE_TO_BACKUP))
    parser.add_argument("--delete-source", action="store_true", default=bool(é»˜è®¤_DELETE_SOURCE))
    parser.add_argument("--delete-today", action="store_true", default=bool(é»˜è®¤_DELETE_TODAY))
    parser.add_argument("--check-gap", action="store_true", default=bool(é»˜è®¤_CHECK_GAP))
    parser.add_argument("--sync-hf", action="store_true", default=bool(é»˜è®¤_SYNC_HF))
    parser.add_argument("--gap-ms-depth", type=int, default=int(é»˜è®¤_GAP_MS_DEPTH))
    parser.add_argument("--gap-ms-trade", type=int, default=int(é»˜è®¤_GAP_MS_TRADE))
    parser.add_argument("--gap-samples", type=int, default=int(é»˜è®¤_GAP_SAMPLES))

    parser.add_argument(
        "--auto-fill-trade-from-depth-gaps",
        action="store_true",
        default=bool(é»˜è®¤_AUTO_FILL_TRADE_FROM_DEPTH_GAPS),
    )
    parser.add_argument("--fill-depth-gap-min-ms", type=int, default=int(é»˜è®¤_FILL_DEPTH_GAP_MIN_MS))
    parser.add_argument(
        "--fill-max-gaps-per-symbol-day",
        type=int,
        default=int(é»˜è®¤_FILL_MAX_GAPS_PER_SYMBOL_DAY),
    )
    parser.add_argument("--fill-max-window-ms", type=int, default=int(é»˜è®¤_FILL_MAX_WINDOW_MS))
    args = parser.parse_args(argv)

    input_root = Path(args.input).expanduser().resolve()
    output_root = Path(args.output).expanduser().resolve()

    symbols = _split_csv(args.symbols)
    date_filter = (args.date or "").strip() or None
    dtype_filter = cast(æ•°æ®ç±»å‹ | None, ((args.dtype or "").strip() or None))

    check_gap_enabled = bool(args.check_gap or args.auto_fill_trade_from_depth_gaps)
    groups = _build_groups(input_root, symbols, date_filter, dtype_filter)

    if not groups:
        print(f"æœªæ‰¾åˆ°å¯æ•´ç†çš„æ•°æ®ç›®å½•: {input_root}")
        return 1

    report: dict = {
        "generated_at": datetime.now().isoformat(timespec="seconds"),
        "input_root": str(input_root),
        "output_root": str(output_root),
        "backup_root": str(args.backup_dir),
        "move_to_backup": bool(args.move_to_backup),
        "delete_source": bool(args.delete_source),
        "delete_today": bool(args.delete_today),
        "check_gap": bool(check_gap_enabled),
        "auto_fill_trade_from_depth_gaps": bool(args.auto_fill_trade_from_depth_gaps),
        "groups": [],
        "gap_summaries": [],
        "gap_samples": [],
    }

    report_groups_by_key: dict[tuple[str, str, æ•°æ®ç±»å‹], dict] = {}
    gap_summaries_by_key: dict[tuple[str, str, æ•°æ®ç±»å‹], dict] = {}
    gap_samples_by_key: dict[tuple[str, str, æ•°æ®ç±»å‹], list[dict]] = {}

    _organize_groups(
        groups,
        output_root=output_root,
        overwrite=bool(args.overwrite),
        delete_source=bool(args.delete_source),
        move_to_backup=bool(args.move_to_backup),
        backup_root=Path(args.backup_dir),
        delete_today=bool(args.delete_today),
        check_gap=bool(check_gap_enabled),
        gap_ms_depth=int(args.gap_ms_depth),
        gap_ms_trade=int(args.gap_ms_trade),
        gap_samples_limit=int(args.gap_samples),
        report_groups_by_key=report_groups_by_key,
        gap_summaries_by_key=gap_summaries_by_key,
        gap_samples_by_key=gap_samples_by_key,
    )

    if args.auto_fill_trade_from_depth_gaps:
        gap_samples_flat: list[dict] = []
        for v in gap_samples_by_key.values():
            gap_samples_flat.extend(v)

        plans = _plan_fill_from_depth_gaps(
            gap_samples_flat,
            min_gap_ms=int(args.fill_depth_gap_min_ms),
            max_gaps_per_symbol_day=int(args.fill_max_gaps_per_symbol_day),
            max_window_ms=int(args.fill_max_window_ms),
        )
        if plans:
            for plan in plans:
                symbol = str(plan["symbol"])
                date = str(plan["date"])
                start_ms = int(plan["start_ms"])
                end_ms = int(plan["end_ms"])
                print(f"è§¦å‘è¡¥å…¨ trade: {symbol} {date} {start_ms}->{end_ms}")
                _run_fill_trade(symbol=symbol, start_ms=start_ms, end_ms=end_ms)

            affected_pairs = sorted({(str(p["symbol"]), str(p["date"])) for p in plans})
            for symbol, date in affected_pairs:
                trade_groups = _build_groups(
                    input_root,
                    symbols=[symbol],
                    date_filter=date,
                    dtype_filter=cast(æ•°æ®ç±»å‹, "trade"),
                )
                if not trade_groups:
                    continue
                _organize_groups(
                    trade_groups,
                    output_root=output_root,
                    overwrite=bool(args.overwrite),
                    delete_source=bool(args.delete_source),
                    move_to_backup=bool(args.move_to_backup),
                    backup_root=Path(args.backup_dir),
                    delete_today=bool(args.delete_today),
                    check_gap=bool(check_gap_enabled),
                    gap_ms_depth=int(args.gap_ms_depth),
                    gap_ms_trade=int(args.gap_ms_trade),
                    gap_samples_limit=int(args.gap_samples),
                    report_groups_by_key=report_groups_by_key,
                    gap_summaries_by_key=gap_summaries_by_key,
                    gap_samples_by_key=gap_samples_by_key,
                )

    report["groups"] = [report_groups_by_key[k] for k in sorted(report_groups_by_key.keys())]
    report["gap_summaries"] = [gap_summaries_by_key[k] for k in sorted(gap_summaries_by_key.keys())]
    report["gap_samples"] = []
    for k in sorted(gap_samples_by_key.keys()):
        report["gap_samples"].extend(gap_samples_by_key[k])

    report_path = output_root / "æ•´ç†æŠ¥å‘Š.json"
    report_path.parent.mkdir(parents=True, exist_ok=True)
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")

    # ç”Ÿæˆ Markdown æŠ¥å‘Š
    md_lines = [
        "# ğŸ“Š è¡Œæƒ…æ•°æ®æ•´ç†æŠ¥å‘Š",
        f"- **ç”Ÿæˆæ—¶é—´**: {report['generated_at']}",
        f"- **è¾“å…¥ç›®å½•**: `{report['input_root']}`",
        f"- **è¾“å‡ºç›®å½•**: `{report['output_root']}`",
        "",
        "## 1. æ•´ç†æ¦‚è§ˆ",
        "| å¸ç§ | æ—¥æœŸ | ç±»å‹ | æ–‡ä»¶æ•° | è¾“å‡ºè¡Œæ•° | çŠ¶æ€ |",
        "|---|---|---|---|---|---|",
    ]
    
    for g in report["groups"]:
        status = "âœ… æˆåŠŸ" if g["rows"] > 0 else "âš ï¸ ç©ºæ•°æ®"
        if g["bad_files"]:
            status = "âŒ æœ‰æŸåæ–‡ä»¶"
        md_lines.append(
            f"| {g['symbol']} | {g['date']} | {g['dtype']} | {g['input_files']} | {g['rows']:,} | {status} |"
        )
    
    md_lines.extend([
        "",
        "## 2. è¿ç»­æ€§æ£€æŸ¥ (ç¼ºå£æŠ¥å‘Š)",
        "> **ç¼ºå£å®šä¹‰**: ç›¸é‚»ä¸¤æ¡æ•°æ®çš„æ—¶é—´å·®è¶…è¿‡é˜ˆå€¼ã€‚",
        "",
        "| å¸ç§ | æ—¥æœŸ | ç±»å‹ | é˜ˆå€¼(ms) | ç¼ºå£æ•°é‡ | æœ€å¤§æ–­æ¡£(ç§’) |",
        "|---|---|---|---|---|---|",
    ])
    
    if not report["gap_summaries"]:
        md_lines.append("\n*(æ— ç¼ºå£æˆ–æœªå¼€å¯æ£€æŸ¥)*")
    else:
        for s in report["gap_summaries"]:
            max_gap_sec = round(s["max_gap_ms"] / 1000, 1)
            md_lines.append(
                f"| {s['symbol']} | {s['date']} | {s['dtype']} | {s['gap_threshold_ms']} | {s['gap_count']} | **{max_gap_sec}s** |"
            )

    md_lines.extend([
        "",
        "## 3. è¯¦ç»†ç¼ºå£æ ·æœ¬ (Top 50)",
        "| å¸ç§ | ç±»å‹ | æ—¶é—´ (å‰) | æ—¶é—´ (å) | æ–­æ¡£æ—¶é•¿ |",
        "|---|---|---|---|---|",
    ])

    if not report["gap_samples"]:
        md_lines.append("\n*(æ— è¯¦ç»†æ ·æœ¬)*")
    else:
        for gap in report["gap_samples"]:
            # è½¬æ¢æ—¶é—´æˆ³ä¸ºå¯è¯»æ ¼å¼
            try:
                t1 = datetime.fromtimestamp(gap["prev_exchange_time"] / 1000).strftime('%H:%M:%S.%f')[:-3]
                t2 = datetime.fromtimestamp(gap["next_exchange_time"] / 1000).strftime('%H:%M:%S.%f')[:-3]
            except Exception:
                t1 = str(gap["prev_exchange_time"])
                t2 = str(gap["next_exchange_time"])
                
            duration = round(gap["gap_ms"] / 1000, 3)
            md_lines.append(
                f"| {gap['symbol']} | {gap['dtype']} | {t1} | {t2} | {duration}s |"
            )

    md_path = output_root / "æ•´ç†æŠ¥å‘Š.md"
    md_path.write_text("\n".join(md_lines), encoding="utf-8")
    print(f"å¯è¯»æŠ¥å‘Šå·²ç”Ÿæˆ: {md_path}")

    if args.sync_hf:
        try:
            from hf_sync import sync_to_hf
            print("\nğŸš€ æ­£åœ¨è§¦å‘äº‘ç«¯åŒæ­¥...")
            sync_to_hf()
        except ImportError:
            print("\nâš ï¸ æ— æ³•åŠ è½½ hf_sync.pyï¼Œè·³è¿‡åŒæ­¥ã€‚")
        except Exception as e:
            print(f"\nâŒ åŒæ­¥è¿‡ç¨‹ä¸­å‡ºé”™: {e}")

    return 0


if __name__ == "__main__":
    raise SystemExit(main(sys.argv[1:]))
