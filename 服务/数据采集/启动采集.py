"""
å¸å®‰åˆçº¦ (USDC) é«˜é¢‘è¡Œæƒ…é‡‡é›†æœåŠ¡
Binance USDS-M Futures High-Frequency Data Collector

åŠŸèƒ½ï¼š
1. å®æ—¶é‡‡é›† BTC, ETH, SOL, XRP, BNB çš„ USDC æœ¬ä½æ°¸ç»­åˆçº¦æ•°æ®ã€‚
# 2. è®¢é˜… Depth (å¯é…ç½®æ¡£ä½) å’Œ AggTrade (é€ç¬”æˆäº¤)ã€‚
# 3. ä½¿ç”¨å¼‚æ­¥ IO (asyncio) æ¥æ”¶ï¼Œçº¿ç¨‹æ±  (ThreadPool) å†™å…¥ Parquetã€‚
4. è‡ªåŠ¨æ–­çº¿é‡è¿ï¼Œä¼˜é›…é€€å‡ºã€‚

ä¾èµ–åº“ (è¯·ç¡®ä¿å®‰è£…):
pip install asyncio websockets pandas pyarrow
"""

import fcntl
import asyncio
import json
import logging
import os
import signal
import ssl
import sys
import time
import subprocess
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any

# ==========================================
# 1. é¡¹ç›®è·¯å¾„è‡ªåŠ¨æ³¨å…¥ (Path Injection)
# ==========================================
# è‡ªåŠ¨å®šä½åˆ° Quant_Unified æ ¹ç›®å½•
# å½“å‰æ–‡ä»¶: Quant_Unified/æœåŠ¡/æ•°æ®é‡‡é›†/å¯åŠ¨é‡‡é›†.py
CURRENT_FILE = Path(__file__).resolve()
PROJECT_ROOT = CURRENT_FILE.parents[2] # å‘ä¸Šè·³ 2 å±‚

# å°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥ Python æœç´¢è·¯å¾„
if str(PROJECT_ROOT) not in sys.path:
    sys.path.append(str(PROJECT_ROOT))

# ==========================================
# 2. ä¾èµ–æ£€æŸ¥
# ==========================================
try:
    import websockets
    import pandas as pd
    import pyarrow
except ImportError as e:
    print(f"âŒ ç¼ºå°‘å¿…è¦çš„ä¾èµ–åº“: {e.name}")
    print("è¯·è¿è¡Œ: pip install websockets pandas pyarrow supabase psutil")
    sys.exit(1)

# ==========================================
# 2.5 Supabase å¿ƒè·³ç›‘æ§ (Monitoring)
# ==========================================
class HeartbeatManager:
    """
    è´Ÿè´£å‘è¿œç¨‹æ•°æ®åº“å‘é€æœåŠ¡çŠ¶æ€ï¼Œå®ç°â€œç™½å«–â€çº§äº‘ç«¯ç›‘æ§ã€‚
    """
    def __init__(self):
        self.url = os.getenv("SUPABASE_URL")
        # å…¼å®¹å¤šç§ç¯å¢ƒå˜é‡å‘½åæ–¹å¼ï¼šäº‘ç«¯ä¼˜å…ˆç”¨ SERVICE_ROLE æˆ– ANONï¼Œæœ¬åœ°å¼€å‘å¯ç”¨ç®€å†™
        self.key = os.getenv("SUPABASE_SERVICE_ROLE_KEY") or os.getenv("SUPABASE_ANON_KEY") or os.getenv("SUPABASE_KEY")
        self.client = None
        if self.url and self.key:
            try:
                from supabase import create_client
                self.client = create_client(self.url, self.key)
                logger.info("â˜ï¸ å·²æˆåŠŸè¿æ¥åˆ° Supabase ç›‘æ§ä¸­å¿ƒ")
            except Exception as e:
                logger.error(f"âŒ åˆå§‹åŒ– Supabase å®¢æˆ·ç«¯å¤±è´¥: {e}")
        else:
            logger.info("â„¹ï¸ æœªæ£€æµ‹åˆ° SUPABASE_URL/KEYï¼Œç›‘æ§æ•°æ®ä»…è®°å½•åœ¨æœ¬åœ°æ—¥å¿—ä¸­ã€‚")

    async def send_heartbeat(self, status: str, details: Dict[str, Any]):
        """å‘é€å¿ƒè·³ä¿¡å·åˆ°äº‘ç«¯"""
        if not self.client:
            return
        
        try:
            import psutil
            # è¡¥å……ç³»ç»Ÿæ€§èƒ½ä¿¡æ¯
            details.update({
                "cpu_percent": psutil.cpu_percent(),
                "memory_percent": psutil.virtual_memory().percent,
                "local_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            })
            
            # æ‰§è¡Œæ•°æ®æ›´æ–° (Upsert)
            data = {
                "service_name": "market_collector",
                "status": status,
                "details": details,
                "updated_at": "now()"
            }
            
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥çš„ Supabase è°ƒç”¨ï¼Œé¿å…å¡ä½å¼‚æ­¥å¾ªç¯
            def _upsert():
                return self.client.table("service_status").upsert(data).execute()

            await asyncio.get_running_loop().run_in_executor(None, _upsert)
        except Exception as e:
            logger.debug(f"âš ï¸ å‘é€å¿ƒè·³ä¿¡å·å¤±è´¥ (éè‡´å‘½é”™è¯¯): {e}")

# ==========================================
# 3. é…ç½®åŒºåŸŸ
# =======================================# å¯¼å…¥å…¨å±€é…ç½®
try:
    from config import DEPTH_LEVEL
except ImportError:
    # å°è¯•ä» Quant_Unified åŒ…å¯¼å…¥ (å¦‚æœè¿è¡Œæ–¹å¼ä¸åŒ)
    try:
        from Quant_Unified.config import DEPTH_LEVEL
    except ImportError:
        print("âš ï¸ æœªæ‰¾åˆ°å…¨å±€é…ç½® config.DEPTH_LEVELï¼Œä½¿ç”¨é»˜è®¤å€¼ 20")
        DEPTH_LEVEL = 20

SYMBOLS = ["BTCUSDC", "ETHUSDC", "SOLUSDC", "XRPUSDC", "BNBUSDC"]

BASE_URL = "wss://fstream.binance.com/stream?streams={}"

# æ•°æ®å­˜å‚¨è·¯å¾„
DATA_DIR = PROJECT_ROOT / "data" / "è¡Œæƒ…æ•°æ®"
DATA_DIR.mkdir(parents=True, exist_ok=True)

# æ—¥å¿—è·¯å¾„
LOG_DIR = PROJECT_ROOT / "ç³»ç»Ÿæ—¥å¿—"
LOG_DIR.mkdir(parents=True, exist_ok=True)

# ç¼“å†²é…ç½®
BUFFER_SIZE_TRIGGER = 5000  # å•ä¸ªç¼“å†²åŒºç§¯ç´¯å¤šå°‘æ¡æ•°æ®è§¦å‘å†™å…¥
FLUSH_INTERVAL = 60         # æ— è®ºæ•°æ®å¤šå°‘ï¼Œæ¯éš”å¤šå°‘ç§’å¼ºåˆ¶å†™å…¥ä¸€æ¬¡

# é‡è¿é…ç½®
MAX_RECONNECT_DELAY = 30    # æœ€å¤§é‡è¿ç­‰å¾…æ—¶é—´(ç§’)

# è‡ªåŠ¨æ•´ç†é…ç½®
AUTO_ORGANIZE_ENABLED = True
AUTO_ORGANIZE_CHECK_INTERVAL_SEC = 600
AUTO_ORGANIZE_FRAGMENT_THRESHOLD = 120
AUTO_ORGANIZE_LOOKBACK_DAYS = 7
AUTO_ORGANIZE_DELETE_SOURCE = True  # è‡ªåŠ¨åˆ é™¤æºç¢æ–‡ä»¶ï¼ˆä»Šæ—¥æ–‡ä»¶é™¤å¤–ï¼Œé™¤éæ‰‹åŠ¨æŒ‡å®šï¼‰

# è‡ªåŠ¨è¡¥å…¨é…ç½®ï¼ˆåŸºäº depth ç¼ºå£æ¨æ–­é‡‡é›†å™¨åœæœºçª—å£ï¼Œä»…è¡¥å…¨ tradeï¼‰
AUTO_FILL_TRADE_FROM_DEPTH_GAPS_ENABLED = True
AUTO_FILL_DEPTH_GAP_MIN_MS = 60_000
AUTO_FILL_MAX_GAPS_PER_SYMBOL_DAY = 3
AUTO_FILL_MAX_WINDOW_MS = 6 * 60 * 60 * 1000

# ==========================================
# 4. æ—¥å¿—é…ç½®
# ==========================================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(LOG_DIR / "market_collector.log", encoding='utf-8')
    ]
)
logger = logging.getLogger("æ•°æ®é‡‡é›†å™¨")

# ==========================================
# 5. æ•°æ®å­˜å‚¨å¼•æ“ (Storage Engine)
# ==========================================

class DataStorageEngine:
    """
    è´Ÿè´£æ•°æ®çš„å†…å­˜ç¼“å†²å’Œç£ç›˜å†™å…¥ã€‚
    æ¶ˆè´¹è€…æ¨¡å¼ï¼šåœ¨ç‹¬ç«‹çš„çº¿ç¨‹æ± ä¸­æ‰§è¡Œå†™å…¥ï¼Œä¸å¡ WebSocketã€‚
    """
    def __init__(self, output_dir: Path):
        self.output_dir = output_dir
        # æ•°æ®ç¼“å†²åŒº: { 'BTCUSDC': { 'depth': [], 'trade': [] }, ... }
        self.buffers: Dict[str, Dict[str, List[Dict]]] = {
            s: {'depth': [], 'trade': []} for s in SYMBOLS
        }
        self.last_flush_time = time.time()
        # çº¿ç¨‹æ± ï¼šç”¨äºæ‰§è¡Œ CPU å¯†é›†å‹å’Œ IO å¯†é›†å‹çš„ Parquet å†™å…¥
        self.io_executor = ThreadPoolExecutor(max_workers=4)
        self.lock = asyncio.Lock() # åç¨‹é”

    def buffer_data(self, symbol: str, data_type: str, record: Dict[str, Any]):
        """ç”Ÿäº§æ•°æ®ï¼šæ”¾å…¥å†…å­˜é˜Ÿåˆ—"""
        self.buffers[symbol][data_type].append(record)

    def check_flush_condition(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ»¡è¶³å†™å…¥æ¡ä»¶"""
        now = time.time()
        # æ¡ä»¶1: æ—¶é—´åˆ°äº†
        if now - self.last_flush_time >= FLUSH_INTERVAL:
            return True
        
        # æ¡ä»¶2: ä»»æ„ä¸€ä¸ªç¼“å†²åŒºæ»¡äº†
        for symbol in SYMBOLS:
            for dtype in ['depth', 'trade']:
                if len(self.buffers[symbol][dtype]) >= BUFFER_SIZE_TRIGGER:
                    return True
        return False

    async def flush(self, force: bool = False):
        """
        è§¦å‘æ•°æ®è½ç›˜ (Consumer)
        """
        # å¦‚æœæ²¡è·å–åˆ°é”ï¼Œè¯´æ˜æ­£åœ¨å†™å…¥ï¼Œè·³è¿‡æœ¬æ¬¡æ£€æŸ¥ï¼ˆé™¤éå¼ºåˆ¶ï¼‰
        if self.lock.locked() and not force:
            return

        async with self.lock:
            if not force and not self.check_flush_condition():
                return

            tasks = []
            current_time = time.time()
            
            # éå†ç¼“å†²åŒºï¼Œå–å‡ºæ•°æ®ï¼Œæ¸…ç©ºç¼“å†²åŒº
            for symbol in SYMBOLS:
                for dtype in ['depth', 'trade']:
                    data_chunk = self.buffers[symbol][dtype]
                    if not data_chunk:
                        continue
                    
                    # åŸå­äº¤æ¢ï¼šå…ˆæŠŠå¼•ç”¨æ‹¿å‡ºæ¥ï¼Œç«‹åˆ»æ¸…ç©ºåŸåˆ—è¡¨
                    # è¿™æ ·ä¸»çº¿ç¨‹å¯ä»¥ç»§ç»­å¾€ buffers é‡Œå¡æ–°æ•°æ®ï¼Œäº’ä¸å½±å“
                    to_write = data_chunk
                    self.buffers[symbol][dtype] = []
                    
                    # å°†å†™å…¥ä»»åŠ¡æ‰”ç»™çº¿ç¨‹æ± 
                    tasks.append(
                        asyncio.get_running_loop().run_in_executor(
                            self.io_executor,
                            self._write_parquet,
                            symbol,
                            dtype,
                            to_write
                        )
                    )
            
            if tasks:
                logger.info(f"âš¡ è§¦å‘æ‰¹é‡å†™å…¥ (Force={force}, Tasks={len(tasks)})...")
                # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆå†™å…¥
                await asyncio.gather(*tasks)
                self.last_flush_time = current_time
                logger.info("âœ… æ‰¹é‡å†™å…¥å®Œæˆ")

    def _write_parquet(self, symbol: str, data_type: str, data: List[Dict]):
        """
        [é˜»å¡å‡½æ•°] åœ¨çº¿ç¨‹ä¸­è¿è¡Œã€‚
        """
        try:
            if not data:
                return

            df = pd.DataFrame(data)
            
            # ç”Ÿæˆè·¯å¾„: ./data/è¡Œæƒ…æ•°æ®/BTCUSDC/2025-12-20/
            today_str = datetime.now().strftime('%Y-%m-%d')
            save_dir = self.output_dir / symbol / today_str
            save_dir.mkdir(parents=True, exist_ok=True)

            # æ–‡ä»¶å: trade_1698372312123456.parquet (çº³ç§’æ—¶é—´æˆ³é˜²æ­¢é‡å)
            timestamp_ns = time.time_ns()
            filename = f"{data_type}_{timestamp_ns}.parquet"
            file_path = save_dir / filename

            # å†™å…¥ Parquet (Snappy å‹ç¼©)
            df.to_parquet(str(file_path), engine='pyarrow', compression='snappy', index=False)
            
        except Exception as e:
            logger.error(f"âŒ å†™å…¥æ–‡ä»¶å¤±è´¥ {symbol} {data_type}: {e}")

# ==========================================
# 6. é‡‡é›†æ ¸å¿ƒ (Collector)
# ==========================================

class BinanceRecorder:
    def __init__(self):
        self.running = True
        self.storage = DataStorageEngine(DATA_DIR)
        self.heartbeat = HeartbeatManager() # åˆå§‹åŒ–ç›‘æ§

        self._auto_organize_last_run: Dict[tuple[str, str], float] = {}
        self._auto_organize_guard = asyncio.Lock()

        ssl_verify_env = os.getenv('BINANCE_WS_SSL_VERIFY')
        self.ssl_verify = ((ssl_verify_env or 'true').lower() != 'false')
        self._allow_insecure_ssl_fallback = (ssl_verify_env is None)
        self._insecure_ssl_fallback_used = False
        
        # --- æ™ºèƒ½è¯Šæ–­é…ç½® ---
        self.consecutive_failures = 0
        self.last_ip_check_time = 0
        # å¸å®‰é™åˆ¶æˆ–éƒ¨åˆ†é™åˆ¶çš„åœ°åŒºä»£ç  (ISO 3166-1 alpha-2)
        self.RESTRICTED_REGIONS = {
            'US': 'ç¾å›½ (United States)',
            'CN': 'ä¸­å›½å†…åœ° (Mainland China)',
            'GB': 'è‹±å›½ (United Kingdom)',
            'CA': 'åŠ æ‹¿å¤§ (Canada)',
            'HK': 'é¦™æ¸¯ (Hong Kong)',
            'JP': 'æ—¥æœ¬ (Japan)',
            'IT': 'æ„å¤§åˆ© (Italy)',
            'DE': 'å¾·å›½ (Germany)',
            'NL': 'è·å…° (Netherlands)',
        }

        self.ssl_context = None
        ca_file = os.getenv('BINANCE_WS_CA_FILE')
        if ca_file and os.path.exists(ca_file):
            try:
                self.ssl_context = ssl.create_default_context(cafile=ca_file)
                logger.info(f"å·²åŠ è½½è‡ªå®šä¹‰ CA è¯ä¹¦: {ca_file}")
            except Exception as e:
                logger.error(f"åŠ è½½è‡ªå®šä¹‰ CA è¯ä¹¦å¤±è´¥: {e}")
                self.ssl_context = None
        elif not self.ssl_verify:
            ctx = ssl.create_default_context()
            ctx.check_hostname = False
            ctx.verify_mode = ssl.CERT_NONE
            self.ssl_context = ctx
            logger.warning("å·²å…³é—­ WebSocket SSL è¯ä¹¦æ ¡éªŒ (BINANCE_WS_SSL_VERIFY=false)")
        
        # æ„é€  Combined Stream URL
        # æ ¼å¼: btcusdc@depth5@100ms / btcusdc@aggTrade
        streams = []
        for s in SYMBOLS:
            lower_s = s.lower()
            streams.append(f"{lower_s}@depth{DEPTH_LEVEL}@100ms")
            streams.append(f"{lower_s}@aggTrade")
        
        self.url = BASE_URL.format("/".join(streams))
        logger.info(f"è®¢é˜… {len(SYMBOLS)} ä¸ªå¸ç§ï¼Œå…± {len(streams)} ä¸ªæ•°æ®æµ")
        logger.info(f"æ•°æ®å­˜æ”¾ç›®å½•: {DATA_DIR}")

    async def _get_current_ip_info(self) -> Dict[str, Any]:
        """è·å–å½“å‰ IP çš„åœ°ç†ä½ç½®ä¿¡æ¯"""
        # é¿å…é¢‘ç¹æŸ¥è¯¢ IP æ¥å£ (è‡³å°‘é—´éš” 60 ç§’)
        now = time.time()
        if now - self.last_ip_check_time < 60:
            return {}
        
        self.last_ip_check_time = now
        url = "http://ip-api.com/json/?fields=status,message,countryCode,query"
        
        def _fetch_blocking():
            import urllib.request
            try:
                # æ˜¾å¼ç¦ç”¨ä»£ç†è¿›è¡Œ IP æ£€æŸ¥ï¼Œä»¥è·å–çœŸå®çš„å‡ºå£ IP (æˆ–è€…æ ¹æ®éœ€è¦å†³å®šæ˜¯å¦å¸¦ä»£ç†)
                # è¿™é‡Œæˆ‘ä»¬ä¿æŒç³»ç»Ÿé»˜è®¤ï¼Œè¿™æ ·å¦‚æœæ˜¯ VPN/ä»£ç†åˆ‡æ¢ï¼Œèƒ½æŸ¥åˆ°åˆ‡æ¢åçš„ IP
                with urllib.request.urlopen(url, timeout=5) as response:
                    return json.loads(response.read().decode())
            except Exception as e:
                return {"status": "fail", "message": str(e)}

        return await asyncio.get_running_loop().run_in_executor(None, _fetch_blocking)

    async def _diagnose_connection_issue(self, error_msg: str = ""):
        """è¯Šæ–­è¿æ¥é—®é¢˜å¹¶ç»™å‡ºå»ºè®®"""
        logger.info("ğŸ” æ­£åœ¨å¯åŠ¨æ™ºèƒ½è¿æ¥è¯Šæ–­...")
        ip_info = await self._get_current_ip_info()
        
        if not ip_info or ip_info.get("status") != "success":
            logger.warning(f"âš ï¸ è¯Šæ–­å¤±è´¥: æ— æ³•è·å– IP åœ°ç†ä½ç½®ä¿¡æ¯ ({ip_info.get('message', 'æœªçŸ¥é”™è¯¯')})")
            return

        current_ip = ip_info.get("query", "æœªçŸ¥")
        country_code = ip_info.get("countryCode", "æœªçŸ¥")
        country_name = self.RESTRICTED_REGIONS.get(country_code, country_code)

        logger.info(f"ğŸ“ å½“å‰å‡ºå£ IP: {current_ip} | å½’å±åœ°: {country_name}")

        # åœºæ™¯ 1: åœ°ç†ä½ç½®å—é™
        if country_code in self.RESTRICTED_REGIONS:
            logger.error("ğŸ›‘ [è¯Šæ–­ç»“æœ] ä¸¥é‡ï¼šå½“å‰ IP å½’å±åœ°å¤„äºå¸å®‰é™åˆ¶åœ°åŒºï¼")
            logger.error(f"   åŸå› : å¸å®‰ä¸æ”¯æŒæ¥è‡ª {country_name} çš„ç›´æ¥ API è®¿é—®ã€‚")
            logger.error("   å»ºè®®: è¯·åˆ‡æ¢ VPN/ä»£ç†è‡³æ–°åŠ å¡ã€æ—¥æœ¬æˆ–å…¶ä»–ä¸å—é™åœ°åŒºã€‚")
        
        # åœºæ™¯ 2: æ•è·åˆ° 403 é”™è¯¯
        elif "403" in error_msg:
            logger.error("ğŸ›‘ [è¯Šæ–­ç»“æœ] è®¿é—®è¢«å°é” (Forbidden 403)")
            logger.error("   åŸå› : ä½ çš„ IP å¯èƒ½å·²è¢«å¸å®‰æš‚æ—¶å±è”½æˆ–å› ä¸ºåœ°åŒºæ”¿ç­–åŸå› è¢«æ‹¦æˆªã€‚")
            logger.error("   å»ºè®®: å³ä¾¿å½’å±åœ°çœ‹ä¼¼æ­£å¸¸ï¼Œä¹Ÿè¯·å°è¯•æ›´æ¢ä»£ç†èŠ‚ç‚¹ã€‚")
        
        # åœºæ™¯ 3: è¿ç»­å¤±è´¥å¤šæ¬¡
        elif self.consecutive_failures >= 5:
            logger.warning("ğŸ›‘ [è¯Šæ–­ç»“æœ] æŒç»­è¿æ¥è¶…æ—¶æˆ–å¤±è´¥")
            logger.info("   å»ºè®®: è¯·æ£€æŸ¥ä½ çš„æœ¬åœ°ç½‘ç»œè¿æ¥æ˜¯å¦ç¨³å®šï¼Œæˆ–è€…å°è¯•é‡å¯ä»£ç†æœåŠ¡ã€‚")

    def _get_proxy_env(self) -> Dict[str, str]:
        keys = [
            'ALL_PROXY', 'all_proxy',
            'HTTPS_PROXY', 'https_proxy',
            'HTTP_PROXY', 'http_proxy',
        ]
        env = {}
        for k in keys:
            v = os.environ.get(k)
            if v:
                env[k] = v
        return env

    def _disable_proxy_env(self):
        for k in [
            'ALL_PROXY', 'all_proxy',
            'HTTPS_PROXY', 'https_proxy',
            'HTTP_PROXY', 'http_proxy',
        ]:
            os.environ.pop(k, None)
        os.environ['NO_PROXY'] = '*'
        os.environ['no_proxy'] = '*'

    def _socks_proxy_configured(self) -> bool:
        env = self._get_proxy_env()
        for v in env.values():
            low = str(v).strip().lower()
            if low.startswith(('socks5://', 'socks5h://', 'socks4://', 'socks://')):
                return True
        return False

    async def _connect_ws(self):
        proxy_env = self._get_proxy_env()
        use_direct = False

        if self._socks_proxy_configured():
            try:
                import python_socks  # noqa: F401
            except Exception:
                use_direct = True
                proxy_view = ", ".join([f"{k}={v}" for k, v in proxy_env.items()])
                logger.error(
                    "æ£€æµ‹åˆ°ä½ è®¾ç½®äº† SOCKS ä»£ç†ï¼Œä½†å½“å‰ç¯å¢ƒç¼ºå°‘ python-socksï¼Œå¯¼è‡´ WebSocket æ— æ³•è¿æ¥ã€‚"
                    "å·²è‡ªåŠ¨ä¸´æ—¶ç¦ç”¨ä»£ç†ï¼Œæ”¹ä¸ºç›´è¿ã€‚è‹¥ä½ å¿…é¡»èµ°ä»£ç†ï¼Œè¯·å…ˆå®‰è£…: pip install python-socks\n"
                    f"å½“å‰ä»£ç†ç¯å¢ƒå˜é‡: {proxy_view}"
                )

        if use_direct:
            self._disable_proxy_env()

        async def _do_connect():
            kwargs = {
                'ping_interval': 20,
                'ping_timeout': 20,
            }
            if self.ssl_context is not None:
                kwargs['ssl'] = self.ssl_context

            try:
                return await websockets.connect(self.url, proxy=None, **kwargs)
            except TypeError:
                return await websockets.connect(self.url, **kwargs)

        try:
            return await _do_connect()
        except Exception as e:
            msg = str(e)
            
            # æ‰©å±•ï¼šä¸ä»…æ•è·è¯ä¹¦é”™è¯¯ï¼Œä¹Ÿæ•è· HTTP 400 (é€šå¸¸ä¹Ÿæ˜¯ä»£ç†/é˜²ç«å¢™å¯¼è‡´çš„æ¡æ‰‹å¤±è´¥)
            is_ssl_error = 'CERTIFICATE_VERIFY_FAILED' in msg
            is_handshake_error = 'HTTP 400' in msg or 'InvalidStatusCode' in msg

            if (
                self.ssl_verify
                and self.ssl_context is None
                and getattr(self, '_allow_insecure_ssl_fallback', False)
                and not getattr(self, '_insecure_ssl_fallback_used', False)
                and (is_ssl_error or is_handshake_error)
            ):
                ctx = ssl.create_default_context()
                ctx.check_hostname = False
                ctx.verify_mode = ssl.CERT_NONE
                self.ssl_context = ctx
                self._insecure_ssl_fallback_used = True
                
                reason = "SSL è¯ä¹¦æ ¡éªŒå¤±è´¥" if is_ssl_error else "HTTP 400 æ¡æ‰‹å¼‚å¸¸"
                logger.warning(
                    f"âš ï¸ {reason}ï¼Œå·²è‡ªåŠ¨æ”¹ä¸ºä¸æ ¡éªŒ SSL ç»§ç»­è¿æ¥ã€‚"
                    "è‹¥è¦æ¢å¤å®‰å…¨æ ¡éªŒï¼šè®¾ç½® BINANCE_WS_CA_FILE=/path/to/ca.pemï¼Œ"
                    "æˆ–è®¾ç½® BINANCE_WS_SSL_VERIFY=true å¼ºåˆ¶æ ¡éªŒã€‚"
                )
                return await _do_connect()
            raise

    def _parse_depth(self, payload: Dict) -> Dict:
        """
        æ¸…æ´— depth5 æ•°æ®
        """
        ts_recv = time.time()
        # T: Transaction Time (æ’®åˆæ—¶é—´)
        ts_exch = payload.get('T', payload.get('E', 0)) 
        
        item = {
            'timestamp': ts_recv,
            'exchange_time': ts_exch,
            'symbol': payload['s']
        }

        # å±•å¹³ Bids (ä¹°å•)
        bids = payload.get('b', [])
        for i in range(DEPTH_LEVEL):
            if i < len(bids):
                item[f'bid{i+1}_p'] = float(bids[i][0])
                item[f'bid{i+1}_q'] = float(bids[i][1])
            else:
                item[f'bid{i+1}_p'] = None
                item[f'bid{i+1}_q'] = None

        # å±•å¹³ Asks (å–å•)
        asks = payload.get('a', [])
        for i in range(DEPTH_LEVEL):
            if i < len(asks):
                item[f'ask{i+1}_p'] = float(asks[i][0])
                item[f'ask{i+1}_q'] = float(asks[i][1])
            else:
                item[f'ask{i+1}_p'] = None
                item[f'ask{i+1}_q'] = None
        
        return item

    def _parse_agg_trade(self, payload: Dict) -> Dict:
        """
        æ¸…æ´— aggTrade æ•°æ®
        """
        return {
            'timestamp': time.time(),
            'exchange_time': payload['T'],
            'symbol': payload['s'],
            'price': float(payload['p']),
            'qty': float(payload['q']),
            'is_buyer_maker': payload['m'] # True=å–æ–¹ä¸»åŠ¨, False=ä¹°æ–¹ä¸»åŠ¨
        }

    def _count_parquet_files(self, symbol: str, date: str) -> int:
        p = DATA_DIR / symbol / date
        if not p.exists():
            return 0
        try:
            return len(list(p.glob("*.parquet")))
        except Exception:
            return 0

    def _iter_candidate_dates(self) -> list[str]:
        today = datetime.now().date()
        cutoff = today - timedelta(days=int(AUTO_ORGANIZE_LOOKBACK_DAYS))

        dates: set[str] = set()
        for symbol in SYMBOLS:
            symbol_dir = DATA_DIR / symbol
            if not symbol_dir.exists():
                continue
            for date_dir in symbol_dir.iterdir():
                if not date_dir.is_dir():
                    continue
                d = date_dir.name
                try:
                    day = datetime.strptime(d, "%Y-%m-%d").date()
                except Exception:
                    continue
                if day > today:  # åªæ’é™¤æœªæ¥çš„æ—¥æœŸï¼ˆå…è®¸æ•´ç†ä»Šå¤©ï¼‰
                    continue
                if day < cutoff:
                    continue
                dates.add(d)

        return sorted(dates)

    async def _run_organize(self, date: str, symbols_csv: str) -> dict | None:
        cmd = [
            sys.executable,
            str(CURRENT_FILE.parent / "æ•´ç†è¡Œæƒ…æ•°æ®.py"),
            "--date",
            date,
            "--symbols",
            symbols_csv,
            "--check-gap",
            "--overwrite",
        ]
        if AUTO_ORGANIZE_DELETE_SOURCE:
            cmd.append("--delete-source")

        def _run_blocking():
            return subprocess.run(cmd, check=False)

        await asyncio.get_running_loop().run_in_executor(None, _run_blocking)

        report_path = PROJECT_ROOT / "data" / "è¡Œæƒ…æ•°æ®_æ•´ç†" / "æ•´ç†æŠ¥å‘Š.json"
        try:
            return json.loads(report_path.read_text(encoding="utf-8"))
        except Exception:
            return None

    async def _run_fill_trade(self, symbol: str, start_ms: int, end_ms: int) -> None:
        if start_ms >= end_ms:
            return
        cmd = [
            sys.executable,
            str(CURRENT_FILE.parent / "è¡¥å…¨å†å²æˆäº¤.py"),
            "--symbol",
            symbol,
            "--start-ms",
            str(int(start_ms)),
            "--end-ms",
            str(int(end_ms)),
        ]

        def _run_blocking():
            return subprocess.run(cmd, check=False)

        await asyncio.get_running_loop().run_in_executor(None, _run_blocking)

    async def _run_auto_organize(self):
        logger.info("ğŸ“… å¯åŠ¨è‡ªåŠ¨æ•´ç†å®ˆæŠ¤...")

        while self.running:
            if not AUTO_ORGANIZE_ENABLED:
                await asyncio.sleep(int(AUTO_ORGANIZE_CHECK_INTERVAL_SEC))
                continue

            try:
                async with self._auto_organize_guard:
                    now_ts = time.time()
                    candidates = self._iter_candidate_dates()

                    for date in candidates:
                        need_symbols: list[str] = []
                        for symbol in SYMBOLS:
                            frag_count = self._count_parquet_files(symbol, date)
                            if frag_count < int(AUTO_ORGANIZE_FRAGMENT_THRESHOLD):
                                continue
                            last = self._auto_organize_last_run.get((symbol, date), 0.0)
                            if now_ts - last < 3600:
                                continue
                            need_symbols.append(symbol)

                        if not need_symbols:
                            continue

                        symbols_csv = ",".join(need_symbols)
                        logger.info(
                            f"ğŸ§¹ è§¦å‘è‡ªåŠ¨æ•´ç†: date={date}, symbols={symbols_csv}, threshold={AUTO_ORGANIZE_FRAGMENT_THRESHOLD}"
                        )
                        report = await self._run_organize(date=date, symbols_csv=symbols_csv)
                        for s in need_symbols:
                            self._auto_organize_last_run[(s, date)] = now_ts

                        if not (AUTO_FILL_TRADE_FROM_DEPTH_GAPS_ENABLED and report):
                            continue

                        gap_samples = report.get("gap_samples") or []
                        depth_gaps = [
                            g
                            for g in gap_samples
                            if g.get("dtype") == "depth" and int(g.get("gap_ms", 0)) >= int(AUTO_FILL_DEPTH_GAP_MIN_MS)
                        ]
                        if not depth_gaps:
                            continue

                        by_symbol: Dict[str, list[dict]] = {}
                        for g in depth_gaps:
                            sym = str(g.get("symbol") or "")
                            if not sym:
                                continue
                            by_symbol.setdefault(sym, []).append(g)

                        for sym, gaps in by_symbol.items():
                            gaps_sorted = sorted(gaps, key=lambda x: int(x.get("gap_ms", 0)), reverse=True)
                            for g in gaps_sorted[: int(AUTO_FILL_MAX_GAPS_PER_SYMBOL_DAY)]:
                                start_ms = int(g["prev_exchange_time"]) + 1
                                end_ms = int(g["next_exchange_time"]) - 1
                                if end_ms - start_ms > int(AUTO_FILL_MAX_WINDOW_MS):
                                    end_ms = start_ms + int(AUTO_FILL_MAX_WINDOW_MS)

                                logger.info(f"ğŸ§© è§¦å‘è¡¥å…¨ trade: {sym} {date} {start_ms}->{end_ms}")
                                await self._run_fill_trade(symbol=sym, start_ms=start_ms, end_ms=end_ms)

                            logger.info(f"ğŸ” è¡¥å…¨åå¤æ•´ç† trade: {sym} {date}")
                            await self._run_organize(date=date, symbols_csv=sym)

            except Exception as e:
                logger.error(f"è‡ªåŠ¨æ•´ç†å®ˆæŠ¤å¼‚å¸¸: {e}")

            await asyncio.sleep(int(AUTO_ORGANIZE_CHECK_INTERVAL_SEC))

    async def _run_heartbeat(self):
        """å®šæœŸå‘é€ç›‘æ§å¿ƒè·³"""
        while self.running:
            try:
                # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
                details = {
                    "symbols": SYMBOLS,
                    "depth_level": DEPTH_LEVEL,
                    "consecutive_failures": self.consecutive_failures,
                    "data_dir": str(DATA_DIR)
                }
                await self.heartbeat.send_heartbeat("RUNNING", details)
            except Exception as e:
                logger.debug(f"å¿ƒè·³å®ˆæŠ¤å¼‚å¸¸: {e}")
            await asyncio.sleep(60) # æ¯åˆ†é’Ÿä¸€æ¬¡

    async def connect(self):
        """ä¸»è¿æ¥å¾ªç¯ (å«æ–­çº¿é‡è¿)"""
        # 1. é™éŸ³ websockets åº“çš„ INFO æ—¥å¿—ï¼Œé˜²æ­¢é‡è¿æ—¶æ§åˆ¶å°åˆ·å±
        logging.getLogger("websockets").setLevel(logging.WARNING)

        asyncio.create_task(self._run_auto_organize())
        asyncio.create_task(self._run_heartbeat())

        retry_delay = 1
        
        while self.running:
            # è®°å½•å°è¯•è¿æ¥çš„æ—¶é—´ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦ä¸º"æŠ–åŠ¨"è¿æ¥
            connect_start_time = time.time()
            
            try:
                logger.info(f"ğŸ“¡ æ­£åœ¨è¿æ¥å¸å®‰åˆçº¦ WebSocket...")
                async with await self._connect_ws() as ws:
                    logger.info("ğŸŸ¢ è¿æ¥æˆåŠŸ! å¼€å§‹æ¥æ”¶æ•°æ®...")
                    
                    # âš ï¸ æ³¨æ„ï¼šæ­¤å¤„ä¸å†ç«‹å³é‡ç½® retry_delay = 1
                    # æˆ‘ä»¬æ”¹ä¸ºåœ¨è¿æ¥æ–­å¼€æ—¶ï¼Œåˆ¤æ–­"è¿™æ¬¡è¿æ¥å­˜æ´»äº†å¤šä¹…"ã€‚
                    # åªæœ‰å­˜æ´»æ—¶é—´ > 10ç§’ï¼Œæ‰åˆ¤å®šä¸ºç½‘ç»œç¨³å®šï¼Œé‡ç½®å»¶è¿Ÿã€‚
                    # è¿™æ ·å¯ä»¥å®Œç¾è§£å†³ IP åˆ‡æ¢æ—¶"è¿ä¸Šå³æ–­"å¯¼è‡´çš„æ— é™æŠ¥é”™åˆ·å±é—®é¢˜ã€‚
                    
                    while self.running:
                        try:
                            # 1ç§’è¶…æ—¶ï¼Œç¡®ä¿èƒ½å®šæœŸé†’æ¥æ£€æŸ¥ flush å’Œ running çŠ¶æ€
                            message = await asyncio.wait_for(ws.recv(), timeout=1.0)
                            data = json.loads(message)
                            
                            # åªè¦æ”¶åˆ°æœ‰æ•ˆæ•°æ®ï¼Œå°±è®¤ä¸ºè¿æ¥æ˜¯é€šçš„ï¼Œé‡ç½®å¤±è´¥è®¡æ•°
                            self.consecutive_failures = 0
                            
                            if 'data' not in data:
                                continue
                                
                            payload = data['data']
                            stream_name = data['stream']
                            
                            # åˆ†å‘å¤„ç†
                            if 'depth5' in stream_name:
                                clean_data = self._parse_depth(payload)
                                self.storage.buffer_data(clean_data['symbol'], 'depth', clean_data)
                            elif 'aggTrade' in stream_name:
                                clean_data = self._parse_agg_trade(payload)
                                self.storage.buffer_data(clean_data['symbol'], 'trade', clean_data)
                                
                        except asyncio.TimeoutError:
                            pass # è¶…æ—¶åªæ˜¯ä¸ºäº†è®©å¾ªç¯è½¬èµ·æ¥ï¼Œæ£€æŸ¥ flush
                        except websockets.exceptions.ConnectionClosed as e:
                            # è¿æ¥å·²æ–­å¼€ï¼Œå¿…é¡»è·³å‡ºå†…å±‚å¾ªç¯ï¼Œè®©å¤–å±‚å¾ªç¯é‡æ–°è¿æ¥
                            # è¿™é‡Œæˆ‘ä»¬ç”¨ raise æŠŠå¼‚å¸¸å‘ä¸ŠæŠ›ï¼Œè®©å¤–å±‚çš„ except æ•è·
                            raise
                        except Exception as e:
                            # å…¶ä»–å¼‚å¸¸ï¼ˆå¦‚ JSON è§£æé”™è¯¯ï¼‰åªæ‰“å°æ—¥å¿—ï¼Œä¸ä¸­æ–­å¾ªç¯
                            logger.error(f"å¤„ç†æ¶ˆæ¯å¼‚å¸¸: {e}")
                        
                        # æ¯æ¬¡å¾ªç¯éƒ½æ£€æŸ¥æ˜¯å¦éœ€è¦å†™å…¥ç¡¬ç›˜
                        await self.storage.flush()

            except (websockets.exceptions.ConnectionClosed, OSError) as e:
                # === æ™ºèƒ½é€€é¿é€»è¾‘ ===
                alive_duration = time.time() - connect_start_time
                self.consecutive_failures += 1
                
                if alive_duration > 15:
                    retry_delay = 1
                
                msg = str(e)
                # è§¦å‘æ™ºèƒ½è¯Šæ–­çš„æ¡ä»¶ï¼šæ•è·åˆ° 403 é”™è¯¯ï¼Œæˆ–è€…è¿ç»­å¤±è´¥ 5 æ¬¡
                do_diagnose = "403" in msg or self.consecutive_failures >= 5
                
                if "CERTIFICATE_VERIFY_FAILED" in msg:
                    logger.error("ğŸ”´ SSL è¯ä¹¦æ ¡éªŒå¤±è´¥...")
                else:
                    logger.warning(f"ğŸ”´ è¿æ¥æ–­å¼€ (å­˜æ´» {alive_duration:.1f}s, ç¬¬{self.consecutive_failures}æ¬¡å¤±è´¥): {e}")

                if do_diagnose:
                    await self._diagnose_connection_issue(msg)

                if not self.running:
                    break
                
                logger.info(f"â³ {retry_delay}ç§’åé‡è¿...")
                await asyncio.sleep(retry_delay)
                retry_delay = min(retry_delay * 2, MAX_RECONNECT_DELAY)

            except Exception as e:
                # === æ™ºèƒ½é€€é¿é€»è¾‘ ===
                alive_duration = time.time() - connect_start_time
                self.consecutive_failures += 1
                if alive_duration > 15:
                    retry_delay = 1

                msg = str(e)
                do_diagnose = "403" in msg or self.consecutive_failures >= 5

                if "python-socks is required" in msg:
                    logger.error("âŒ ç¼ºå°‘ python-socks åº“...")
                elif "CERTIFICATE_VERIFY_FAILED" in msg:
                    logger.error(f"âŒ SSL è¯ä¹¦é—®é¢˜: {msg}")
                else:
                    logger.error(f"âŒ æœªçŸ¥é”™è¯¯ (å­˜æ´» {alive_duration:.1f}s, ç¬¬{self.consecutive_failures}æ¬¡å¤±è´¥): {e}")
                
                if do_diagnose:
                    await self._diagnose_connection_issue(msg)

                logger.info(f"â³ {retry_delay}ç§’åé‡è¿...")
                await asyncio.sleep(retry_delay)
                retry_delay = min(retry_delay * 2, MAX_RECONNECT_DELAY)

    async def shutdown(self):
        """ä¼˜é›…é€€å‡º"""
        logger.info("ğŸ›‘ æ­£åœ¨åœæ­¢é‡‡é›†å™¨ï¼Œè¯·ç¨å€™...")
        self.running = False
        # å¼ºåˆ¶åˆ·å†™å‰©ä½™æ•°æ®
        await self.storage.flush(force=True)
        # å…³é—­çº¿ç¨‹æ± 
        self.storage.io_executor.shutdown(wait=True)
        logger.info("ğŸ‘‹ å†è§ã€‚")

# ==========================================
# 7. ä¸»ç¨‹åºå…¥å£
# ==========================================

async def main():
    # --- å•ä¾‹é”æ£€æŸ¥ (é˜²é‡å¤å¯åŠ¨) ---
    lock_file_path = DATA_DIR / "market_collector.lock"
    try:
        # æ‰“å¼€é”æ–‡ä»¶ï¼ˆå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºï¼‰
        lock_file = open(lock_file_path, 'w')
        # å°è¯•è·å–éé˜»å¡æ’ä»–é”
        # LOCK_EX: æ’ä»–é” (Exclusive Lock)
        # LOCK_NB: éé˜»å¡ (Non-Blocking)ï¼Œå¦‚æœå·²è¢«é”ä½åˆ™ç«‹å³æŠ›å¼‚å¸¸
        fcntl.lockf(lock_file, fcntl.LOCK_EX | fcntl.LOCK_NB)
        
        # å†™å…¥å½“å‰è¿›ç¨‹ IDï¼Œæ–¹ä¾¿è°ƒè¯•ï¼ˆå¯é€‰ï¼‰
        lock_file.write(str(os.getpid()))
        lock_file.flush()
        
        # æ³¨æ„ï¼šä¸è¦å…³é—­ lock_fileï¼Œä¹Ÿä¸è¦ fcntl.LOCK_UNï¼Œ
        # ç›´åˆ°ç¨‹åºé€€å‡ºï¼ˆæ“ä½œç³»ç»Ÿä¼šè‡ªåŠ¨é‡Šæ”¾é”ï¼‰ã€‚
        # å¦‚æœåœ¨è¿™é‡Œ close äº†ï¼Œé”å°±å¤±æ•ˆäº†ã€‚
        # æˆ‘ä»¬æŠŠ lock_file å¼•ç”¨æŒ‚åœ¨ loop ä¸Šé˜²æ­¢è¢«åƒåœ¾å›æ”¶ï¼ˆè™½ç„¶ main å‡½æ•°ä¸é€€å‡ºä¹Ÿè¡Œï¼‰
        
    except (IOError, BlockingIOError):
        logger.warning(f"âš ï¸ ç¨‹åºå·²åœ¨è¿è¡Œä¸­ (é”æ–‡ä»¶å ç”¨: {lock_file_path})")
        logger.warning("æ— éœ€é‡å¤å¯åŠ¨ã€‚è‹¥ç¡®ä¿¡æ— ç¨‹åºè¿è¡Œï¼Œè¯·åˆ é™¤è¯¥é”æ–‡ä»¶åé‡è¯•ã€‚")
        # ä¼˜é›…é€€å‡º
        sys.exit(0)
    # ----------------------------

    recorder = BinanceRecorder()
    
    # æ³¨å†Œä¿¡å·å¤„ç† (Ctrl+C)
    loop = asyncio.get_running_loop()
    stop_event = asyncio.Event()
    
    def signal_handler():
        logger.info("æ”¶åˆ°é€€å‡ºä¿¡å· (SIGINT/SIGTERM)...")
        stop_event.set()

    # æ³¨å†Œä¿¡å·ï¼ˆWindows ä¸‹å¯èƒ½ä¸æ”¯æŒ add_signal_handlerï¼Œéœ€ç‰¹æ®Šå¤„ç†ï¼Œè¿™é‡Œé»˜è®¤ Unix/Macï¼‰
    if sys.platform != 'win32':
        for sig in (signal.SIGINT, signal.SIGTERM):
            loop.add_signal_handler(sig, signal_handler)
    else:
        logger.info("Windowsç¯å¢ƒ: è¯·æŒ‰ Ctrl+C è§¦å‘ KeyboardInterrupt")

    # å¯åŠ¨é‡‡é›†ä»»åŠ¡
    collector_task = asyncio.create_task(recorder.connect())
    
    # ç­‰å¾…é€€å‡ºä¿¡å·
    try:
        if sys.platform == 'win32':
            # Windows ä¸‹ç®€å•çš„ç­‰å¾…ï¼Œä¾é å¤–å±‚ KeyboardInterrupt æ•è·
            while not stop_event.is_set():
                await asyncio.sleep(1)
        else:
            await stop_event.wait()
    except asyncio.CancelledError:
        pass
    
    # æ‰§è¡Œæ¸…ç†
    await recorder.shutdown()
    collector_task.cancel()
    try:
        await collector_task
    except asyncio.CancelledError:
        pass

if __name__ == "__main__":
    try:
        # Windowsä¸‹å¯èƒ½éœ€è¦è®¾ç½® SelectorEventLoop
        if sys.platform == 'win32':
            asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
            
        asyncio.run(main())
    except KeyboardInterrupt:
        # å†æ¬¡æ•è·ä»¥é˜²ä¸‡ä¸€
        pass
